"""
Privacy Settings API Routes
Controls search engine indexing and crawler access
"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import PlainTextResponse
from pydantic import BaseModel
from typing import List, Optional
from core.database import db
from core.auth import get_admin_user
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/admin/privacy", tags=["Privacy Settings"])
public_router = APIRouter(tags=["Public"])

# Collection for privacy settings
COLLECTION = "site_privacy_settings"


class PathRule(BaseModel):
    path: str
    action: str  # "allow" or "disallow"


class PrivacySettings(BaseModel):
    block_search_engines: bool = True  # Default: blocked (private site)
    custom_rules: List[PathRule] = []
    sitemap_enabled: bool = False
    custom_robots_txt: Optional[str] = None


# Default settings for a private site
DEFAULT_SETTINGS = {
    "id": "default",
    "block_search_engines": True,
    "custom_rules": [
        {"path": "/admin", "action": "disallow"},
        {"path": "/api", "action": "disallow"},
    ],
    "sitemap_enabled": False,
    "custom_robots_txt": None
}


async def get_privacy_settings() -> dict:
    """Get current privacy settings or return defaults"""
    settings = await db[COLLECTION].find_one({"id": "default"}, {"_id": 0})
    if not settings:
        return DEFAULT_SETTINGS.copy()
    return settings


@router.get("")
async def get_settings(admin=Depends(get_admin_user)):
    """Get current privacy settings"""
    settings = await get_privacy_settings()
    return settings


@router.put("")
async def update_settings(
    settings: PrivacySettings,
    admin=Depends(get_admin_user)
):
    """Update privacy settings"""
    try:
        update_data = {
            "id": "default",
            **settings.dict()
        }
        
        await db[COLLECTION].update_one(
            {"id": "default"},
            {"$set": update_data},
            upsert=True
        )
        
        logger.info(f"Privacy settings updated by admin {admin.get('email')}")
        return {"success": True, "message": "Privacy settings updated"}
    except Exception as e:
        logger.error(f"Error updating privacy settings: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/reset")
async def reset_settings(admin=Depends(get_admin_user)):
    """Reset privacy settings to defaults"""
    try:
        await db[COLLECTION].update_one(
            {"id": "default"},
            {"$set": DEFAULT_SETTINGS},
            upsert=True
        )
        logger.info(f"Privacy settings reset to defaults by admin {admin.get('email')}")
        return {"success": True, "message": "Privacy settings reset to defaults"}
    except Exception as e:
        logger.error(f"Error resetting privacy settings: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== PUBLIC ROBOTS.TXT ENDPOINT ==============

@public_router.get("/robots.txt", response_class=PlainTextResponse)
async def get_robots_txt():
    """
    Dynamic robots.txt based on privacy settings.
    This endpoint should be served at the root of the domain.
    """
    settings = await get_privacy_settings()
    
    # If custom robots.txt is provided, use it directly
    if settings.get("custom_robots_txt"):
        return settings["custom_robots_txt"]
    
    lines = ["# Generated by ChiPi Link Privacy Settings", ""]
    
    if settings.get("block_search_engines", True):
        # Block all crawlers
        lines.extend([
            "User-agent: *",
            "Disallow: /",
            "",
            "# Search engines are blocked for this private site",
        ])
    else:
        # Allow crawlers with custom rules
        lines.append("User-agent: *")
        
        # Apply custom rules
        custom_rules = settings.get("custom_rules", [])
        for rule in custom_rules:
            action = "Disallow" if rule.get("action") == "disallow" else "Allow"
            lines.append(f"{action}: {rule.get('path', '/')}")
        
        # If no rules defined and not blocking, allow everything
        if not custom_rules:
            lines.append("Allow: /")
        
        lines.append("")
        
        # Add sitemap if enabled
        if settings.get("sitemap_enabled"):
            lines.append("Sitemap: /sitemap.xml")
    
    return "\n".join(lines)


# ============== META TAG HELPER ENDPOINT ==============

@public_router.get("/privacy/meta-robots")
async def get_meta_robots():
    """
    Returns the appropriate meta robots directive.
    Frontend can use this to add meta tags dynamically.
    """
    settings = await get_privacy_settings()
    
    if settings.get("block_search_engines", True):
        return {
            "content": "noindex, nofollow, noarchive, nosnippet",
            "blocked": True
        }
    else:
        return {
            "content": "index, follow",
            "blocked": False
        }
